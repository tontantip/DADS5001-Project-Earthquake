{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "032efc26-c676-4005-9f9e-60215063aad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢: 2248 ‡πÅ‡∏ñ‡∏ß | ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢: 4317 ‡πÅ‡∏ñ‡∏ß\n",
      "üìä ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô CV Macro-F1 (6 ‡∏Ñ‡∏•‡∏≤‡∏™, group-aware):\n",
      "              Macro-F1 (6 ‡∏Ñ‡∏•‡∏≤‡∏™)\n",
      "XGB                    0.946563\n",
      "RF                     0.863923\n",
      "Ridge                  0.830022\n",
      "MI                     0.670828\n",
      "‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç           0.614804\n",
      "\n",
      "‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (6 ‡∏Ñ‡∏•‡∏≤‡∏™): XGB\n",
      "üîí ‡πÄ‡∏Å‡∏ì‡∏ë‡πå(Thresholds) policy-lock: [0.20724839898564215, 0.3258889165172987, 0.44001517176695937, 0.6472635707526015] | CV Macro-F1(6 ‡∏Ñ‡∏•‡∏≤‡∏™) = 0.9706\n",
      "üß™ Nested-CV Macro-F1 (threshold mode: policy-lock + target-quantile, min_gap=0.08): 0.6952\n",
      "üîÅ Bootstrap thresholds mean: [0.2055, 0.3251, 0.4335, 0.6487] | std: [0.0022, 0.0017, 0.0173, 0.0209]\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢ (‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå):\n",
      "DMG_LEVEL_FINAL\n",
      "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢    4317\n",
      "‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å             1029\n",
      "‡∏ô‡πâ‡∏≠‡∏¢                 663\n",
      "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á              281\n",
      "‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á               179\n",
      "‡∏ß‡∏¥‡∏Å‡∏§‡∏ï                 96\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìÅ ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏µ‡∏ï‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢ (‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏µ‡∏ï/‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢): C:\\Users\\piriy\\Desktop\\dataset_with_final_weights_groupaware_1.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# -----------------------------\n",
    "# XGBoost (‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏•‡πâ‡∏°‡πÅ‡∏°‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á)\n",
    "# -----------------------------\n",
    "try:\n",
    "    from xgboost import XGBClassifier  # pip install xgboost\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "# =============================\n",
    "# PATH / Utils / Labels / Flags\n",
    "# =============================\n",
    "file_path = r\"C:\\Users\\piriy\\Desktop\\dataset.xlsx\"\n",
    "out_path  = r\"C:\\Users\\piriy\\Desktop\\dataset_with_final_weights_groupaware_1.xlsx\"\n",
    "\n",
    "RANDOM_SEED = 20250913\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# --- Production flags & Policies ---\n",
    "ENABLE_NESTED_CV_CHECK   = True     # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Nested-CV ‡∏Ç‡∏≠‡∏á threshold tuning (‡∏Ñ‡∏ß‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤)\n",
    "MIN_GAP                  = 0.08     # ‡∏•‡∏î‡∏Å‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ threshold ‡πÅ‡∏Ç‡πá‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\n",
    "COARSE_STEP              = 0.15     # coarse grid (‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏≠‡∏ô‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö policy-lock)\n",
    "POLICY_LOCK_QUANTILES    = True     # ‡πÉ‡∏ä‡πâ q20/40/60/80 (‡∏´‡∏£‡∏∑‡∏≠ target-quantile) ‡∏à‡∏≤‡∏Å‡∏ù‡∏±‡πà‡∏á damage ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á\n",
    "USE_TARGET_PREVALENCE    = True     # ‡πÉ‡∏ä‡πâ \"‡πÄ‡∏õ‡πâ‡∏≤‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô\" ‡∏ï‡πà‡∏≠‡∏ä‡∏±‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏°‡∏†‡∏≤‡∏£‡∏∞‡∏á‡∏≤‡∏ô (‡∏ñ‡πâ‡∏≤ False ‡∏à‡∏∞‡πÉ‡∏ä‡πâ q20/40/60/80)\n",
    "OPTIMIZE_FOR             = \"f1\"     # \"f1\" ‡∏´‡∏£‡∏∑‡∏≠ \"cost\" (‡∏ñ‡πâ‡∏≤ \"cost\" ‡∏à‡∏∞‡πÉ‡∏ä‡πâ COST_MATRIX ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á)\n",
    "\n",
    "# ‡πÄ‡∏õ‡πâ‡∏≤‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ù‡∏±‡πà‡∏á \"‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\") ‡∏£‡∏ß‡∏° 100\n",
    "# [‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å, ‡∏ô‡πâ‡∏≠‡∏¢, ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á, ‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á, ‡∏ß‡∏¥‡∏Å‡∏§‡∏ï]\n",
    "# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: top2 = 8%+4% = 12%\n",
    "TARGET_PREVALENCE = np.array([46, 30, 12, 8, 4], dtype=float)\n",
    "\n",
    "# Cost matrix ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏°‡∏î \"cost\" (‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡∏´‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏•‡∏≤‡∏î‡∏ä‡∏±‡πâ‡∏ô‡∏ö‡∏ô)\n",
    "# ‡πÅ‡∏Å‡∏ô‡πÅ‡∏ñ‡∏ß = true, ‡πÅ‡∏Å‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå = pred ; ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™ = labels_6_order ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á\n",
    "COST_MATRIX = np.array([\n",
    "# pred:   0    1    2    3    4    5\n",
    "          [0,   1,   2,   3,   4,   5],  # true 0 (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢)\n",
    "          [1,   0,   1,   2,   3,   4],  # true 1\n",
    "          [2,   1,   0,   1,   2,   3],  # true 2\n",
    "          [3,   2,   1,   0,   2,   3],  # true 3\n",
    "          [5,   4,   3,   2,   0,   2],  # true 4\n",
    "          [8,   6,   5,   4,   2,   0],  # true 5 (‡∏ß‡∏¥‡∏Å‡∏§‡∏ï)\n",
    "], dtype=float)\n",
    "\n",
    "labels_5 = [\"‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å\", \"‡∏ô‡πâ‡∏≠‡∏¢\", \"‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á\", \"‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á\", \"‡∏ß‡∏¥‡∏Å‡∏§‡∏ï\"]\n",
    "labels_6_order = [\"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\"] + labels_5\n",
    "code_map = {\"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\": 0, \"‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å\": 1, \"‡∏ô‡πâ‡∏≠‡∏¢\": 2, \"‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á\": 3, \"‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á\": 4, \"‡∏ß‡∏¥‡∏Å‡∏§‡∏ï\": 5}\n",
    "\n",
    "# ---- sanitize thresholds (‡∏°‡∏µ‡∏Å‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ö‡∏≤‡∏á) & ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ----\n",
    "def sanitize_thresholds(th, eps=1e-6, min_gap=MIN_GAP):\n",
    "    th = np.asarray(th, dtype=float).tolist()\n",
    "    th = [min(max(0.0, t), 1.0) for t in th]  # clip [0,1]\n",
    "    th = sorted(th)\n",
    "\n",
    "    # ‡πÄ‡∏î‡∏¥‡∏ô‡∏´‡∏ô‡πâ‡∏≤: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ min_gap\n",
    "    for i in range(1, len(th)):\n",
    "        need = th[i-1] + max(eps, min_gap)\n",
    "        if th[i] < need:\n",
    "            th[i] = min(1.0, need)\n",
    "\n",
    "    # ‡πÑ‡∏•‡πà‡∏Å‡∏•‡∏±‡∏ö: ‡∏Ñ‡∏á min_gap ‡∏à‡∏≤‡∏Å‡∏Ç‡∏ß‡∏≤‡πÑ‡∏õ‡∏ã‡πâ‡∏≤‡∏¢\n",
    "    for i in range(len(th)-2, -1, -1):\n",
    "        need = th[i+1] - max(eps, min_gap)\n",
    "        if th[i] > need:\n",
    "            th[i] = max(0.0, need)\n",
    "    return th\n",
    "\n",
    "THRESHOLDS = sanitize_thresholds([0.20, 0.40, 0.60, 0.80])  # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á\n",
    "\n",
    "# ---- nudge_top2: ‡∏à‡∏π‡∏ô t3/t4 ‡πÉ‡∏´‡πâ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô top2 ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ----\n",
    "def nudge_top2(th, X01, mask_dmg, target_pct=12.0, tol=0.2, step=0.002, max_iter=800):\n",
    "    \"\"\"\n",
    "    ‡∏Ç‡∏¢‡∏±‡∏ö t3/t4 ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏±‡∏ô‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô top2 (‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á+‡∏ß‡∏¥‡∏Å‡∏§‡∏ï) ‡πÉ‡∏´‡πâ‡πÉ‡∏Å‡∏•‡πâ target_pct (‡∏ö‡∏ô‡∏ù‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢)\n",
    "    - ‡∏Ç‡∏∂‡πâ‡∏ô t3/t4  -> ‡∏•‡∏î top2\n",
    "    - ‡∏•‡∏á t3/t4    -> ‡πÄ‡∏û‡∏¥‡πà‡∏° top2\n",
    "    \"\"\"\n",
    "    th = np.array(sanitize_thresholds(th), dtype=float)\n",
    "    X01 = np.asarray(X01, dtype=float)\n",
    "    mb = np.asarray(mask_dmg, dtype=bool)\n",
    "    Xd = X01[mb]\n",
    "    if Xd.size == 0:\n",
    "        return th.tolist()\n",
    "\n",
    "    def top2_pct(th_):\n",
    "        lab = bin_by_thresholds(Xd, thresholds=th_, labels=labels_5)  # pandas.Categorical\n",
    "        arr = np.asarray(lab, dtype=object)                            # -> ndarray (object)\n",
    "        p = np.isin(arr, [\"‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á\", \"‡∏ß‡∏¥‡∏Å‡∏§‡∏ï\"]).mean() * 100.0         # ‡πÉ‡∏ä‡πâ np.isin ‡πÅ‡∏ó‡∏ô .isin\n",
    "        return float(p)\n",
    "\n",
    "    cur = top2_pct(th)\n",
    "    for _ in range(max_iter):\n",
    "        if abs(cur - target_pct) <= tol:\n",
    "            break\n",
    "        if cur > target_pct:   # ‡∏°‡∏≤‡∏Å‡πÑ‡∏õ ‚Üí ‡∏î‡∏±‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "            th[3] = min(1.0, th[3] + step)                   # ‡∏Ç‡∏¢‡∏±‡∏ö t4 ‡∏Å‡πà‡∏≠‡∏ô\n",
    "            th[2] = min(th[3] - MIN_GAP, th[2] + step/2)     # ‡∏£‡∏±‡∏Å‡∏©‡∏≤ MIN_GAP\n",
    "        else:                  # ‡∏ô‡πâ‡∏≠‡∏¢‡πÑ‡∏õ ‚Üí ‡∏ú‡πà‡∏≠‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏•‡∏á\n",
    "            th[3] = max(th[2] + MIN_GAP, th[3] - step)\n",
    "            th[2] = max(th[1] + MIN_GAP, th[2] - step/2)\n",
    "        th = np.array(sanitize_thresholds(th), dtype=float)\n",
    "        cur = top2_pct(th)\n",
    "    return th.tolist()\n",
    "\n",
    "\n",
    "def normalize_weights(w: np.ndarray) -> np.ndarray:\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    w[w < 0] = 0.0\n",
    "    s = w.sum()\n",
    "    return (w / s) if s > 0 else np.ones_like(w) / len(w)\n",
    "\n",
    "def new_splitter(X, y, groups):\n",
    "    unique_groups = getattr(groups, \"nunique\", lambda: pd.Series(groups).nunique())()\n",
    "    if unique_groups >= 5:\n",
    "        return GroupKFold(n_splits=5).split(X, y, groups)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏°‡∏µ‡πÄ‡∏û‡∏µ‡∏¢‡∏á {unique_groups} < 5 ‚Üí ‡πÉ‡∏ä‡πâ StratifiedKFold(5)\")\n",
    "        return StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(X, y)\n",
    "\n",
    "# -------- robust CV helper (‡πÉ‡∏ä‡πâ LogisticRegression ‡πÄ‡∏õ‡πá‡∏ô meta-evaluator) --------\n",
    "def macro_f1_cv_robust(X: np.ndarray, y: np.ndarray, groups: pd.Series) -> float:\n",
    "    splitter = list(new_splitter(X, y, groups))\n",
    "    scores = []\n",
    "    for tr_idx, va_idx in splitter:\n",
    "        X_tr, X_va = X[tr_idx], X[va_idx]\n",
    "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "        if np.unique(y_va).shape[0] < 2:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        clf = LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=RANDOM_SEED,\n",
    "            solver=\"lbfgs\",\n",
    "        )\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        y_pred = clf.predict(X_va)\n",
    "        f1 = f1_score(y_va, y_pred, average=\"macro\", zero_division=0)\n",
    "        scores.append(float(f1))\n",
    "    return float(np.mean(scores)) if len(scores) > 0 else 0.0\n",
    "\n",
    "# -------- ‡πÉ‡∏ä‡πâ robust CV ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô weights --------\n",
    "def evaluate_weights(w: np.ndarray, X, y, groups) -> float:\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    if w.shape[0] != X.shape[1]:\n",
    "        raise ValueError(f\"weight length={len(w)} != n_features={X.shape[1]}\")\n",
    "    dmg = X @ w\n",
    "    mn, mx = np.nanmin(dmg), np.nanmax(dmg)\n",
    "    dmg_norm = (dmg - mn) / (mx - mn) if mx > mn else np.zeros_like(dmg)\n",
    "    X_feat = dmg_norm.reshape(-1, 1)\n",
    "    return macro_f1_cv_robust(X_feat, np.asarray(y, dtype=int), groups)\n",
    "\n",
    "def bin_by_thresholds(arr01: np.ndarray, thresholds=THRESHOLDS, labels=labels_5) -> pd.Categorical:\n",
    "    \"\"\"\n",
    "    ‡∏ï‡∏¥‡∏î‡∏õ‡πâ‡∏≤‡∏¢ 5 ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ö‡∏ô‡∏™‡πÄ‡∏Å‡∏• [0,1]: [0,t1) [t1,t2) [t2,t3) [t3,t4) [t4,1]\n",
    "    \"\"\"\n",
    "    thresholds = sanitize_thresholds(thresholds)\n",
    "    arr = np.nan_to_num(np.asarray(arr01, dtype=float), nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    bins = np.digitize(arr, thresholds, right=False)  # 0..len(thresholds)\n",
    "    bins = np.clip(bins, 0, len(labels)-1)\n",
    "    lab = np.array(labels, dtype=object)[bins]\n",
    "    return pd.Categorical(lab, categories=labels, ordered=True)\n",
    "\n",
    "# =============================\n",
    "# Load + Check\n",
    "# =============================\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "base_cols = [\"UP_DMG\", \"WALL_DMG\", \"FLOOR_DMG\", \"PILLAR_DMG\", \"STRUCTURE_\"]\n",
    "missing = [c for c in base_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: {missing}\")\n",
    "\n",
    "df[base_cols] = df[base_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "# =============================\n",
    "# ‡πÅ‡∏¢‡∏Å‡πÅ‡∏ñ‡∏ß‡∏°‡∏µ/‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\n",
    "# =============================\n",
    "mask_has_damage = df[base_cols].sum(axis=1) > 0\n",
    "print(f\"‚úÖ ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢: {int(mask_has_damage.sum())} ‡πÅ‡∏ñ‡∏ß | ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢: {int((~mask_has_damage).sum())} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "# =============================\n",
    "# Normalize: q95 ‡∏à‡∏≤‡∏Å‡∏ù‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢ ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏Å‡πâ‡∏≠‡∏ô\n",
    "# =============================\n",
    "norm_q95 = {}\n",
    "df_damage_for_norm = df.loc[mask_has_damage].copy()\n",
    "for c in base_cols:\n",
    "    q95 = df_damage_for_norm[c].quantile(0.95)\n",
    "    q95 = 1.0 if pd.isna(q95) or q95 == 0 else float(q95)\n",
    "    norm_q95[c] = q95\n",
    "    df[f\"{c}_NORM\"] = df[c].clip(upper=q95) / q95\n",
    "\n",
    "norm_cols = [f\"{c}_NORM\" for c in base_cols]\n",
    "df[norm_cols] = df[norm_cols].fillna(0.0)\n",
    "\n",
    "# =============================\n",
    "# Target 6 ‡∏Ñ‡∏•‡∏≤‡∏™ (seed label ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ thresholds ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô)\n",
    "# =============================\n",
    "tmp_damage_score = df.loc[mask_has_damage, norm_cols].mean(axis=1).to_numpy()\n",
    "dmg_levels_5_fixed = bin_by_thresholds(tmp_damage_score, thresholds=THRESHOLDS, labels=labels_5)\n",
    "\n",
    "y6 = pd.Series(index=df.index, dtype=object)\n",
    "y6.loc[~mask_has_damage] = \"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\"\n",
    "y6.loc[mask_has_damage]  = dmg_levels_5_fixed.astype(str)\n",
    "\n",
    "le6 = LabelEncoder()\n",
    "le6.fit(labels_6_order)  # ‡∏•‡πá‡∏≠‡∏Å‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™ (‡πÑ‡∏ó‡∏¢)\n",
    "y6_enc = le6.transform(y6.astype(str))\n",
    "\n",
    "# =============================\n",
    "# ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GroupKFold\n",
    "# =============================\n",
    "if \"DISTRICT\" in df.columns and \"SUB_DISTRI\" in df.columns:\n",
    "    groups_all = df[\"DISTRICT\"].astype(str) + \" | \" + df[\"SUB_DISTRI\"].astype(str)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö DISTRICT ‡∏´‡∏£‡∏∑‡∏≠ SUB_DISTRI ‚Üí ‡∏à‡∏∞‡πÉ‡∏ä‡πâ StratifiedKFold\")\n",
    "    groups_all = pd.Series([\"__all__\"] * len(df))\n",
    "\n",
    "# =============================\n",
    "# ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå + Weights ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ (6 ‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏±‡πâ‡∏á‡∏Å‡πâ‡∏≠‡∏ô)\n",
    "# =============================\n",
    "X_all = df[norm_cols].to_numpy()\n",
    "\n",
    "# 1) Expert prior\n",
    "expert_weights = normalize_weights(np.array([0.15, 0.10, 0.10, 0.25, 0.40]))\n",
    "\n",
    "# 2) Mutual Information\n",
    "mi6 = mutual_info_classif(X_all, y6_enc, random_state=42, discrete_features=False)\n",
    "mi6_weights = normalize_weights(mi6)\n",
    "\n",
    "# 3) Ridge (‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡πà‡∏≤‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡πâ‡∏≤‡∏°‡∏ä‡∏±‡πâ‡∏ô)\n",
    "ridge6 = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "ridge6.fit(X_all, y6_enc)\n",
    "ridge6_coef = ridge6.coef_\n",
    "ridge6_feat = np.abs(ridge6_coef) if ridge6_coef.ndim == 1 else np.abs(ridge6_coef).sum(axis=0)\n",
    "ridge6_weights = normalize_weights(ridge6_feat)\n",
    "\n",
    "# 4) RandomForest\n",
    "rf6 = RandomForestClassifier(n_estimators=300, random_state=42, class_weight=\"balanced\", n_jobs=-1)\n",
    "rf6.fit(X_all, y6_enc)\n",
    "rf6_weights = normalize_weights(rf6.feature_importances_)\n",
    "\n",
    "# 5) XGBoost (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°)\n",
    "if HAS_XGB:\n",
    "    xgb6 = XGBClassifier(\n",
    "        n_estimators=400, learning_rate=0.05, max_depth=3,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        eval_metric=\"mlogloss\", random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    xgb6.fit(X_all, y6_enc)\n",
    "    xgb6_weights = normalize_weights(xgb6.feature_importances_)\n",
    "else:\n",
    "    xgb6_weights = None\n",
    "\n",
    "weights_dict6 = {\n",
    "    \"‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç\": expert_weights,\n",
    "    \"MI\":     mi6_weights,\n",
    "    \"Ridge\":  ridge6_weights,\n",
    "    \"RF\":     rf6_weights,\n",
    "}\n",
    "if xgb6_weights is not None:\n",
    "    weights_dict6[\"XGB\"] = xgb6_weights\n",
    "\n",
    "# =============================\n",
    "# Evaluate weights (6-class, group-aware) ‚Üí Best method\n",
    "# =============================\n",
    "scores6 = {name: evaluate_weights(w, X_all, y6_enc, groups_all) for name, w in weights_dict6.items()}\n",
    "cv_df6 = pd.DataFrame.from_dict(scores6, orient=\"index\", columns=[\"Macro-F1 (6 ‡∏Ñ‡∏•‡∏≤‡∏™)\"]).sort_values(\"Macro-F1 (6 ‡∏Ñ‡∏•‡∏≤‡∏™)\", ascending=False)\n",
    "print(\"üìä ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô CV Macro-F1 (6 ‡∏Ñ‡∏•‡∏≤‡∏™, group-aware):\")\n",
    "print(cv_df6)\n",
    "\n",
    "best_method6 = cv_df6.index[0]\n",
    "final_weights6 = weights_dict6[best_method6]\n",
    "print(f\"\\n‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (6 ‡∏Ñ‡∏•‡∏≤‡∏™): {best_method6}\")\n",
    "\n",
    "# =============================\n",
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏Å‡∏≠‡∏£‡πå‡∏£‡∏ß‡∏°‡∏î‡πâ‡∏ß‡∏¢ weights ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ '‡∏à‡∏π‡∏ô thresholds')\n",
    "# =============================\n",
    "TOTAL_all = X_all @ final_weights6\n",
    "mn_all, mx_all = np.nanmin(TOTAL_all), np.nanmax(TOTAL_all)\n",
    "total_all_norm = (TOTAL_all - mn_all) / (mx_all - mn_all) if mx_all > mn_all else np.zeros_like(TOTAL_all)\n",
    "\n",
    "# --------- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏ï‡∏≤‡∏° thresholds ----------\n",
    "def macro_f1_for_thresholds_full(th):\n",
    "    th = sanitize_thresholds(th)\n",
    "    y6_th = pd.Series(index=df.index, dtype=object)\n",
    "    y6_th.loc[~mask_has_damage] = \"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\"\n",
    "    y6_th.loc[mask_has_damage] = bin_by_thresholds(\n",
    "        total_all_norm[mask_has_damage], thresholds=th, labels=labels_5\n",
    "    ).astype(str)\n",
    "    le = LabelEncoder().fit(labels_6_order)\n",
    "    y_enc = le.transform(y6_th.astype(str))\n",
    "    X_feat = total_all_norm.reshape(-1, 1)\n",
    "    return macro_f1_cv_robust(X_feat, y_enc, groups_all)\n",
    "\n",
    "def expected_cost_for_thresholds(th):\n",
    "    th = sanitize_thresholds(th)\n",
    "    y_pred = pd.Series(\"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\", index=df.index, dtype=object)\n",
    "    y_pred.loc[mask_has_damage] = bin_by_thresholds(\n",
    "        total_all_norm[mask_has_damage], thresholds=th, labels=labels_5\n",
    "    ).astype(str)\n",
    "    enc = LabelEncoder().fit(labels_6_order)\n",
    "    yp = enc.transform(y_pred.astype(str))\n",
    "    yt = enc.transform(y6.astype(str))\n",
    "    return COST_MATRIX[yt, yp].mean()\n",
    "\n",
    "def coarse_grid_candidates(step=COARSE_STEP):\n",
    "    grid = np.arange(step, 1.0, step)\n",
    "    cand = []\n",
    "    for t1 in grid:\n",
    "        for t2 in grid:\n",
    "            if t2 <= t1: continue\n",
    "            for t3 in grid:\n",
    "                if t3 <= t2: continue\n",
    "                for t4 in grid:\n",
    "                    if t4 <= t3: continue\n",
    "                    cand.append([t1, t2, t3, t4])\n",
    "    return cand\n",
    "\n",
    "def local_refine(th_best, n_iter=300, sigma=0.04, objective=\"f1\"):\n",
    "    best = np.array(th_best, dtype=float)\n",
    "    if objective == \"f1\":\n",
    "        best_score = macro_f1_for_thresholds_full(best)\n",
    "        better = lambda s, b: s > b\n",
    "    else:\n",
    "        best_score = expected_cost_for_thresholds(best)\n",
    "        better = lambda s, b: s < b  # minimize cost\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        prop = best + rng.normal(0, sigma, size=4)\n",
    "        prop = sanitize_thresholds(prop)\n",
    "        score = macro_f1_for_thresholds_full(prop) if objective == \"f1\" else expected_cost_for_thresholds(prop)\n",
    "        if better(score, best_score):\n",
    "            best, best_score = np.array(prop), score\n",
    "    return best.tolist(), best_score\n",
    "\n",
    "# ============ 1) ‡∏´‡∏≤ \"‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°\" ‡∏Ç‡∏≠‡∏á thresholds ============\n",
    "if mask_has_damage.any():\n",
    "    if POLICY_LOCK_QUANTILES and USE_TARGET_PREVALENCE:\n",
    "        q = np.cumsum(TARGET_PREVALENCE[:-1] / TARGET_PREVALENCE.sum())\n",
    "        start_th = sanitize_thresholds(np.quantile(total_all_norm[mask_has_damage], q))\n",
    "        # ‡∏à‡∏π‡∏ô‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÉ‡∏´‡πâ top2 ‚âà 12% (‡∏ö‡∏ô‡∏ù‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢)\n",
    "        start_th = nudge_top2(start_th, total_all_norm, mask_has_damage,\n",
    "                              target_pct=12.0, tol=0.2)\n",
    "    else:\n",
    "        q = np.quantile(total_all_norm[mask_has_damage], [0.2, 0.4, 0.6, 0.8])\n",
    "        start_th = sanitize_thresholds(q)\n",
    "else:\n",
    "    start_th = THRESHOLDS\n",
    "\n",
    "# ============ 2) ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ï‡∏≤‡∏° Policy / ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô ============\n",
    "if POLICY_LOCK_QUANTILES:\n",
    "    THRESHOLDS = start_th\n",
    "    if OPTIMIZE_FOR == \"cost\":\n",
    "        score_txt = expected_cost_for_thresholds(THRESHOLDS)\n",
    "        print(f\"üîí ‡πÄ‡∏Å‡∏ì‡∏ë‡πå(Thresholds) policy-lock: {THRESHOLDS} | Expected Cost = {score_txt:.4f}\")\n",
    "    else:\n",
    "        score_txt = macro_f1_for_thresholds_full(THRESHOLDS)\n",
    "        print(f\"üîí ‡πÄ‡∏Å‡∏ì‡∏ë‡πå(Thresholds) policy-lock: {THRESHOLDS} | CV Macro-F1(6 ‡∏Ñ‡∏•‡∏≤‡∏™) = {score_txt:.4f}\")\n",
    "else:\n",
    "    cands = [[*start_th]] + coarse_grid_candidates(step=COARSE_STEP)\n",
    "    if OPTIMIZE_FOR == \"cost\":\n",
    "        best_th, best_score = None, np.inf\n",
    "        for th in cands:\n",
    "            s = expected_cost_for_thresholds(th)\n",
    "            if s < best_score:\n",
    "                best_th, best_score = th, s\n",
    "        best_th, best_score = local_refine(best_th, n_iter=300, sigma=0.04, objective=\"cost\")\n",
    "        THRESHOLDS = sanitize_thresholds(best_th)\n",
    "        print(f\"üéØ ‡πÄ‡∏Å‡∏ì‡∏ë‡πå(Thresholds) ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (min Expected Cost, step={COARSE_STEP}, min_gap={MIN_GAP:.02f}): {THRESHOLDS} | Cost = {best_score:.4f}\")\n",
    "    else:\n",
    "        best_th, best_score = None, -1.0\n",
    "        for th in cands:\n",
    "            s = macro_f1_for_thresholds_full(th)\n",
    "            if s > best_score:\n",
    "                best_th, best_score = th, s\n",
    "        best_th, best_score = local_refine(best_th, n_iter=300, sigma=0.04, objective=\"f1\")\n",
    "        THRESHOLDS = sanitize_thresholds(best_th)\n",
    "        print(f\"üéØ ‡πÄ‡∏Å‡∏ì‡∏ë‡πå(Thresholds) ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô F1, step={COARSE_STEP}, min_gap={MIN_GAP:.02f}): {THRESHOLDS} | CV Macro-F1 = {best_score:.4f}\")\n",
    "\n",
    "# ============ 3) Nested-CV (‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤) ============\n",
    "nested_cv_score = None\n",
    "if ENABLE_NESTED_CV_CHECK:\n",
    "    splitter_outer = list(new_splitter(total_all_norm.reshape(-1,1), le6.transform(y6.astype(str)), groups_all))\n",
    "    scores_nested = []\n",
    "    groups_series = groups_all if isinstance(groups_all, pd.Series) else pd.Series(groups_all)\n",
    "\n",
    "    for tr_idx, va_idx in splitter_outer:\n",
    "        X_tr_norm = total_all_norm[tr_idx]\n",
    "        mask_tr = mask_has_damage.values[tr_idx]\n",
    "        groups_tr = groups_series.iloc[tr_idx]\n",
    "\n",
    "        # ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô threshold ‡∏ö‡∏ô train\n",
    "        if mask_tr.any():\n",
    "            if POLICY_LOCK_QUANTILES and USE_TARGET_PREVALENCE:\n",
    "                qt = np.cumsum(TARGET_PREVALENCE[:-1] / TARGET_PREVALENCE.sum())\n",
    "                start_th0 = sanitize_thresholds(np.quantile(X_tr_norm[mask_tr], qt))\n",
    "            else:\n",
    "                start_th0 = sanitize_thresholds(np.quantile(X_tr_norm[mask_tr], [0.2,0.4,0.6,0.8]))\n",
    "        else:\n",
    "            start_th0 = THRESHOLDS\n",
    "\n",
    "        if POLICY_LOCK_QUANTILES:\n",
    "            best_th0 = start_th0\n",
    "        else:\n",
    "            def score_th_on_train(th):\n",
    "                y_tr_series = pd.Series(\"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\", index=groups_tr.index, dtype=object)\n",
    "                y_tr_series.loc[mask_tr] = bin_by_thresholds(\n",
    "                    X_tr_norm[mask_tr], thresholds=th, labels=labels_5\n",
    "                ).astype(str)\n",
    "                le_tmp = LabelEncoder().fit(labels_6_order)\n",
    "                y_tr_enc = le_tmp.transform(y_tr_series.astype(str))\n",
    "                # ‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î cost ‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏±‡∏ö‡∏´‡∏±‡∏ß‡πÄ‡∏õ‡πá‡∏ô -cost ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ logic ‡πÄ‡∏î‡∏¥‡∏° (maximize)\n",
    "                if OPTIMIZE_FOR == \"cost\":\n",
    "                    yp = le_tmp.transform(y_tr_series.astype(str))\n",
    "                    yt = le_tmp.transform(y6.iloc[tr_idx].astype(str))\n",
    "                    return -COST_MATRIX[yt, yp].mean()\n",
    "                return macro_f1_cv_robust(X_tr_norm.reshape(-1,1), y_tr_enc, groups_tr)\n",
    "\n",
    "            cands0 = [[*start_th0]] + coarse_grid_candidates(step=COARSE_STEP)\n",
    "            best_th0, best_sc0 = None, -np.inf\n",
    "            for th in cands0:\n",
    "                sc = score_th_on_train(th)\n",
    "                if sc > best_sc0:\n",
    "                    best_th0, best_sc0 = th, sc\n",
    "            # refine\n",
    "            obj = \"cost\" if OPTIMIZE_FOR == \"cost\" else \"f1\"\n",
    "            best_th0, _ = local_refine(best_th0, n_iter=150, sigma=0.03, objective=obj)\n",
    "\n",
    "        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ö‡∏ô valid fold\n",
    "        X_va_norm = total_all_norm[va_idx]\n",
    "        mask_va = mask_has_damage.values[va_idx]\n",
    "        groups_va = groups_series.iloc[va_idx]\n",
    "\n",
    "        y_va_pred = pd.Series(\"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\", index=groups_va.index, dtype=object)\n",
    "        y_va_pred.loc[mask_va] = bin_by_thresholds(\n",
    "            X_va_norm[mask_va], thresholds=best_th0, labels=labels_5\n",
    "        ).astype(str)\n",
    "\n",
    "        y_va_true = y6.iloc[va_idx]\n",
    "        le_tmp = LabelEncoder().fit(labels_6_order)\n",
    "\n",
    "        if OPTIMIZE_FOR == \"cost\":\n",
    "            yt = le_tmp.transform(y_va_true.astype(str))\n",
    "            yp = le_tmp.transform(y_va_pred.astype(str))\n",
    "            scores_nested.append(float(-COST_MATRIX[yt, yp].mean()))  # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤\n",
    "        else:\n",
    "            f1_va = f1_score(\n",
    "                le_tmp.transform(y_va_true.astype(str)),\n",
    "                le_tmp.transform(y_va_pred.astype(str)),\n",
    "                average=\"macro\", zero_division=0\n",
    "            )\n",
    "            scores_nested.append(float(f1_va))\n",
    "\n",
    "    nested_cv_score = float(np.mean(scores_nested)) if scores_nested else None\n",
    "    mode_txt = (\"policy-lock + target-quantile\" if (POLICY_LOCK_QUANTILES and USE_TARGET_PREVALENCE)\n",
    "                else (\"policy-lock q20/40/60/80\" if POLICY_LOCK_QUANTILES\n",
    "                      else f\"tuned (step={COARSE_STEP})\"))\n",
    "    metric_txt = \"Macro-F1\" if OPTIMIZE_FOR == \"f1\" else \"-ExpectedCost (‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏≤‡∏Å‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)\"\n",
    "    print(f\"üß™ Nested-CV {metric_txt} (threshold mode: {mode_txt}, min_gap={MIN_GAP:.02f}): {nested_cv_score:.4f}\")\n",
    "\n",
    "# =============================\n",
    "# Bootstrap Stability (‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Ç‡∏≠‡∏á thresholds/‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô)\n",
    "# =============================\n",
    "def bootstrap_threshold_stability(X01, mask_dmg, groups, thresholds, B=200, seed=2025):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    groups = pd.Series(groups)\n",
    "    uniq = groups.unique()\n",
    "    th_list = []\n",
    "    counts = []\n",
    "    for _ in range(B):\n",
    "        sel_g = rng.choice(uniq, size=len(uniq), replace=True)\n",
    "        idx = groups.isin(sel_g).values\n",
    "        Xb = X01[idx]; mb = mask_dmg.values[idx]\n",
    "        if mb.any():\n",
    "            qb = np.quantile(Xb[mb], [0.2,0.4,0.6,0.8]) if not USE_TARGET_PREVALENCE else \\\n",
    "                 np.quantile(Xb[mb], np.cumsum(TARGET_PREVALENCE[:-1] / TARGET_PREVALENCE.sum()))\n",
    "            qb = sanitize_thresholds(qb)\n",
    "        else:\n",
    "            qb = sanitize_thresholds(thresholds)\n",
    "        th_list.append(qb)\n",
    "        # class counts under this qb\n",
    "        lab = pd.Series(\"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\", index=np.arange(Xb.shape[0]), dtype=object)\n",
    "        lab.loc[mb] = bin_by_thresholds(Xb[mb], thresholds=qb, labels=labels_5).astype(str)\n",
    "        counts.append(lab.value_counts().reindex([\"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\"]+labels_5, fill_value=0).to_dict())\n",
    "    th_arr = np.array(th_list)\n",
    "    return th_arr.mean(0), th_arr.std(0), counts\n",
    "\n",
    "th_mean, th_std, boot_counts = bootstrap_threshold_stability(\n",
    "    total_all_norm, mask_has_damage, groups_all, THRESHOLDS, B=200, seed=2025\n",
    ")\n",
    "print(f\"üîÅ Bootstrap thresholds mean: {np.round(th_mean, 4).tolist()} | std: {np.round(th_std, 4).tolist()}\")\n",
    "\n",
    "# =============================\n",
    "# Apply weights ‚Üí Final Score & Class (‡πÉ‡∏ä‡πâ thresholds ‡∏ó‡∏µ‡πà‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ)\n",
    "# =============================\n",
    "df[\"TOTAL_DMG_FINAL\"] = TOTAL_all\n",
    "df[\"TOTAL_DMG_FINAL_NORM\"] = np.nan_to_num(total_all_norm, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "df[\"DMG_LEVEL_FINAL\"] = None\n",
    "df.loc[~mask_has_damage, \"DMG_LEVEL_FINAL\"] = \"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\"\n",
    "df.loc[mask_has_damage, \"DMG_LEVEL_FINAL\"] = bin_by_thresholds(\n",
    "    df.loc[mask_has_damage, \"TOTAL_DMG_FINAL_NORM\"].to_numpy(),\n",
    "    thresholds=THRESHOLDS, labels=labels_5\n",
    ").astype(str)\n",
    "\n",
    "df[\"DMG_CODE\"] = df[\"DMG_LEVEL_FINAL\"].map(code_map).astype(\"Int64\")\n",
    "\n",
    "print(\"\\n‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢ (‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå):\")\n",
    "dist_final = df[\"DMG_LEVEL_FINAL\"].value_counts(dropna=False)\n",
    "print(dist_final)\n",
    "\n",
    "# =============================\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° (‡∏≠‡∏¥‡∏á label seed y6)\n",
    "# =============================\n",
    "le_final = LabelEncoder().fit(labels_6_order)\n",
    "y_true_enc = le_final.transform(y6.astype(str))\n",
    "\n",
    "pred_labels = pd.Series(\"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\", index=df.index, dtype=object)\n",
    "pred_labels.loc[mask_has_damage] = bin_by_thresholds(\n",
    "    df.loc[mask_has_damage, \"TOTAL_DMG_FINAL_NORM\"].to_numpy(),\n",
    "    thresholds=THRESHOLDS, labels=labels_5\n",
    ").astype(str)\n",
    "y_pred_enc = le_final.transform(pred_labels.astype(str))\n",
    "\n",
    "cls_report = classification_report(\n",
    "    y_true_enc, y_pred_enc, target_names=labels_6_order, zero_division=0, output_dict=True\n",
    ")\n",
    "report_df = pd.DataFrame(cls_report).transpose()\n",
    "# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢\n",
    "report_df = report_df.rename(columns={\n",
    "    \"precision\": \"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (Precision)\",\n",
    "    \"recall\": \"‡∏Å‡∏≤‡∏£‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° (Recall)\",\n",
    "    \"f1-score\": \"F1-Score\",\n",
    "    \"support\": \"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\",\n",
    "})\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    y_true_enc, y_pred_enc,\n",
    "    labels=list(range(len(labels_6_order)))\n",
    ")\n",
    "cm_df = pd.DataFrame(cm, index=labels_6_order, columns=labels_6_order)\n",
    "\n",
    "# =============================\n",
    "# ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°: ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å, ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå, ‡πÄ‡∏°‡∏ï‡∏≤, Stability ‡∏™‡∏£‡∏∏‡∏õ\n",
    "# =============================\n",
    "weights_table = pd.DataFrame({\"‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå\": base_cols, \"‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å\": final_weights6})\n",
    "norm_params = pd.DataFrame({\n",
    "    \"‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå\": list(norm_q95.keys()),\n",
    "    \"‡∏Ñ‡πà‡∏≤ q95 ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ\": list(norm_q95.values()),\n",
    "    \"‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏\": [\"‡∏ï‡∏±‡∏î‡∏ö‡∏ô‡∏ó‡∏µ‡πà q95 (‡∏à‡∏≤‡∏Å‡∏ù‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢) ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢ q95\"] * len(norm_q95)\n",
    "})\n",
    "\n",
    "# ‡πÅ‡∏û‡πá‡∏Ñ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î stability ‡∏Ç‡∏≠‡∏á‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏±‡πâ‡∏ô‡∏à‡∏≤‡∏Å bootstrap (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢)\n",
    "def summarize_boot_counts(boot_counts_list):\n",
    "    keys = [\"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\"] + labels_5\n",
    "    arr = np.array([[d[k] for k in keys] for d in boot_counts_list], dtype=float)\n",
    "    mean = arr.mean(0); std = arr.std(0)\n",
    "    total = mean.sum()\n",
    "    pct = mean / total * 100.0\n",
    "    return pd.DataFrame({\n",
    "        \"‡∏ä‡∏±‡πâ‡∏ô\": keys,\n",
    "        \"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (bootstrap)\": np.round(mean, 2),\n",
    "        \"‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô\": np.round(std, 2),\n",
    "        \"‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (%)\": np.round(pct, 2),\n",
    "    })\n",
    "\n",
    "stability_df = summarize_boot_counts(boot_counts)\n",
    "th_stability_df = pd.DataFrame({\n",
    "    \"Threshold\": [\"t1\",\"t2\",\"t3\",\"t4\"],\n",
    "    \"‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (bootstrap)\": np.round(th_mean, 6),\n",
    "    \"‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô\": np.round(th_std, 6),\n",
    "})\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á meta\n",
    "meta = pd.DataFrame({\n",
    "    \"‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (6 ‡∏Ñ‡∏•‡∏≤‡∏™)\": [best_method6],\n",
    "    \"‡πÇ‡∏´‡∏°‡∏î Threshold\": [(\"policy-lock + target-quantile\" if (POLICY_LOCK_QUANTILES and USE_TARGET_PREVALENCE)\n",
    "                    else (\"policy-lock q20/40/60/80\" if POLICY_LOCK_QUANTILES else f\"tuned (step={COARSE_STEP})\"))],\n",
    "    \"‡πÄ‡∏Å‡∏ì‡∏ë‡πå Thresholds\": [str(THRESHOLDS)],\n",
    "    \"‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏´‡∏•‡∏±‡∏Å\": [(\"Macro-F1\" if OPTIMIZE_FOR == \"f1\" else \"Expected Cost (‡∏¢‡∏¥‡πà‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)\")],\n",
    "    \"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô CV Macro-F1 (6 ‡∏Ñ‡∏•‡∏≤‡∏™)\": [float(cv_df6.iloc[0, 0])],\n",
    "    \"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Nested-CV ‡∏´‡∏•‡∏±‡∏Å\": [nested_cv_score if nested_cv_score is not None else np.nan],\n",
    "    \"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\": [len(df)],\n",
    "    \"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\": [int(mask_has_damage.sum())],\n",
    "    \"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\": [int((~mask_has_damage).sum())],\n",
    "    \"‡∏Ñ‡πà‡∏≤ Random Seed\": [RANDOM_SEED],\n",
    "    \"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å\": [\n",
    "        (\"‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á Nested-CV ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å; \"\n",
    "         + (\"‡∏Ñ‡∏∏‡∏°‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏î‡πâ‡∏ß‡∏¢ target-quantile\" if USE_TARGET_PREVALENCE else \"‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≠‡∏ô‡πÑ‡∏ó‡∏•‡πå 20/40/60/80\")\n",
    "         + f\"; min_gap={MIN_GAP:.02f}; optimize_for={OPTIMIZE_FOR}\")\n",
    "    ],\n",
    "})\n",
    "\n",
    "# =============================\n",
    "# Export ‚Üí Excel (‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢)\n",
    "# =============================\n",
    "df_out = df.rename(columns={\n",
    "    \"TOTAL_DMG_FINAL\": \"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\",\n",
    "    \"TOTAL_DMG_FINAL_NORM\": \"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢(‡∏õ‡∏£‡∏±‡∏ö‡∏™‡πÄ‡∏Å‡∏•)\",\n",
    "    \"DMG_LEVEL_FINAL\": \"‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢(‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)\",\n",
    "    \"DMG_CODE\": \"‡∏£‡∏´‡∏±‡∏™‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢\",\n",
    "})\n",
    "\n",
    "with pd.ExcelWriter(out_path) as writer:\n",
    "    df_out.to_excel(writer, sheet_name=\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\", index=False)\n",
    "    weights_table.to_excel(writer, sheet_name=\"‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\", index=False)\n",
    "    cv_df6.to_excel(writer, sheet_name=\"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ôCV-6‡∏Ñ‡∏•‡∏≤‡∏™\")\n",
    "    norm_params.to_excel(writer, sheet_name=\"‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô\", index=False)\n",
    "    meta.to_excel(writer, sheet_name=\"‡πÄ‡∏°‡∏ï‡∏≤‡∏î‡∏≤‡∏ï‡πâ‡∏≤\", index=False)\n",
    "    report_df.to_excel(writer, sheet_name=\"‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å\")\n",
    "    cm_df.to_excel(writer, sheet_name=\"‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå‡∏™‡∏±‡∏ö‡∏™‡∏ô\")\n",
    "    stability_df.to_excel(writer, sheet_name=\"Bootstrap-‡∏ä‡∏±‡πâ‡∏ô\", index=False)\n",
    "    th_stability_df.to_excel(writer, sheet_name=\"Bootstrap-Thresholds\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nüìÅ ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏µ‡∏ï‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢ (‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏µ‡∏ï/‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢): {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb4d692-15f7-4f1d-b9ef-a1e0267e502d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
